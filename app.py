"""
O vasto e complexo mercado de vinhos portugueses, 
com mais de 250 castas aut√≥ctones, 
apresenta um significativo "paradoxo da escolha" para o consumidor. 
A informa√ß√£o dispon√≠vel √© frequentemente dispersa, 
t√©cnica ou comercialmente enviesada, 
dificultando uma decis√£o de compra informada. 
Este artigo apresenta o desenvolvimento e a avalia√ß√£o do VinhoBot, 
uma prova de conceito de um sistema de recomenda√ß√£o conversacional concebido para atuar como um "sommelier digital". 
Para mitigar o risco de "alucina√ß√µes" factuais, 
comuns em Modelos de Linguagem de Grande Escala (LLMs), 
foi implementada uma arquitetura de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG). 
O sistema utiliza uma base de dados PostgreSQL com a extens√£o pgvector para armazenar uma base de conhecimento curada, 
um pipeline de ingest√£o de dados para gerar embeddings vetoriais, 
e uma aplica√ß√£o de chat com Chainlit que interage com a API da Groq. 
A avalia√ß√£o do sistema, 
conduzida por um especialista certificado (WSET 2) sobre um conjunto de 12 perguntas de complexidade vari√°vel, 
revelou uma taxa de sucesso de 48.3% na obten√ß√£o da recomenda√ß√£o exata esperada. 
Os resultados demonstram uma fiabilidade condicional: 
o sistema excele em consultas factuais com crit√©rios bem definidos, 
mas revela limita√ß√µes perante perguntas que exigem a interpreta√ß√£o de conceitos subjetivos ou racioc√≠nio inferencial. 
Conclui-se que, embora promissora, 
a fiabilidade de sistemas RAG em dom√≠nios de nicho depende criticamente do alinhamento entre a 
complexidade da pergunta e a capacidade do pipeline de recupera√ß√£o, 
destacando a necessidade de futuras investiga√ß√µes em pesquisa h√≠brida e modelos de embedding espec√≠ficos do dom√≠nio.
"""

import os
import logging
from dotenv import load_dotenv
from pathlib import Path

# Adicionar a importa√ß√£o do Chainlit
import chainlit as cl
from chainlit.input_widget import Select

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable, RunnablePassthrough

# Modelos
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings

# Postgres
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import DistanceStrategy

# Re-ranking
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from rich.logging import RichHandler

GROQ_MODELS = [
    "openai/gpt-oss-120b",
    "qwen/qwen3-32b",
    "deepseek-r1-distill-llama-70b",
    "moonshotai/kimi-k2-instruct",
    "llama-3.3-70b-versatile",
]

# --- Fun√ß√µes de Configura√ß√£o e Setup ---

def setup_logging():
    """Configura o logging para uma melhor visualiza√ß√£o."""
    logging.basicConfig(
        level="INFO",
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
        force=True,
    )
    return logging.getLogger(__name__)

def find_and_load_dotenv(logger):
    """Procura e carrega o ficheiro .env."""

    project_root = Path(__file__).resolve().parent.parent
    env_path = project_root / "vinhobot" / "compose" / ".env"
    
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        logger.info(f"Ficheiro .env carregado de: {env_path}")
    else:
        project_root = Path.cwd().parent
        env_path = project_root / "vinhobot" / "compose" / ".env"
        if env_path.exists():
            load_dotenv(dotenv_path=env_path)
            logger.info(f"Ficheiro .env carregado de: {env_path}")
        else:
            logger.warning(f"Ficheiro .env n√£o foi encontrado. As vari√°veis de ambiente devem ser definidas manualmente.")

def get_db_config():
    """L√™ as vari√°veis de ambiente da base de dados."""

    db_user = os.getenv("DB_USER", "postgres")
    db_password = os.getenv("DB_PASSWORD", "postgres")
    db_host = os.getenv("DB_HOST", "localhost")
    db_port = os.getenv("DB_PORT", "5432")
    db_name = os.getenv("DB_NAME", "ragdb")
    
    return {
        "db_user": db_user,
        "db_password": db_password,
        "db_host": db_host,
        "db_port": db_port,
        "db_name": db_name,
        "connection": f"postgresql+asyncpg://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    }

def get_app_settings():
    """Retorna as configura√ß√µes da aplica√ß√£o."""

    return {
        "collection_name": "wine_embeddings",
        "embedding_model_name": "all-MiniLM-L6-v2",
        "reranker_model_name": "BAAI/bge-reranker-base",
        "use_reranker": True,
        "retriever_k_for_reranker": 10,
    }

# --- Fun√ß√µes do Pipeline RAG ---

def get_embedding_model(config, logger):
    """Inicializa e retorna o modelo de embedding."""

    logger.info(f"A carregar o modelo de embedding: {config['embedding_model_name']}")
    return HuggingFaceEmbeddings(
      model_name=config['embedding_model_name'],
      model_kwargs={'device': 'cpu'},
      encode_kwargs={'normalize_embeddings': True}
    )

def get_groq_llm(model_name: str, logger):
    """Inicializa e retorna o modelo de linguagem via Groq API."""

    logger.info(f"A inicializar o modelo de linguagem via Groq API: {model_name}")
    if not os.getenv("GROQ_API_KEY"):
        logger.error("A GROQ_API_KEY n√£o foi encontrada. O LLM n√£o pode ser inicializado.")
        raise ValueError("GROQ_API_KEY n√£o definida.")
    return ChatGroq(temperature=0.6, model_name=model_name)

async def get_async_vector_store(embeddings, config, logger):
    """Liga-se e retorna a PGVector store j√° existente de forma ass√≠ncrona."""

    logger.info("A ligar √† Vector Store de forma ass√≠ncrona...")
    try:
        vector_store = PGVector(
            connection=config['connection'],
            collection_name=config['collection_name'],
            embeddings=embeddings,
            distance_strategy=DistanceStrategy.COSINE,
            create_extension=False,
            async_mode=True,
        )

        logger.info("Vector Store conectada com sucesso (modo ass√≠ncrono)")
        return vector_store
    
    except Exception as e:
        logger.error(f"Falha ao inicializar a PGVector: {e}")
        raise

def get_rag_prompt():
    """Cria o ChatPromptTemplate para a cadeia RAG."""
    template = """
    √â um sommelier portugu√™s especialista e amig√°vel. A sua tarefa √© responder √† pergunta do utilizador baseando-se apenas no contexto fornecido.
    Se o contexto n√£o contiver a informa√ß√£o para responder, diga educadamente que n√£o encontrou um vinho com essas caracter√≠sticas nos seus registos.
    Forne√ßa recomenda√ß√µes detalhadas e √∫teis, mencionando o nome do vinho, produtor, regi√£o e as notas de prova relevantes.
    No final da sua resposta, inclua sempre as fontes de informa√ß√£o de onde retirou os dados, se estiverem dispon√≠veis no contexto, sob um t√≠tulo 'Fontes:'.    
    Mantenha a resposta concisa, clara e em portugu√™s de Portugal.

    Contexto:
    {context}

    Pergunta:
    {question}

    Resposta √∫til:
    """
    return ChatPromptTemplate.from_template(template)

def get_retriever(vector_store, config, logger):
    """Cria um retriever com re-ranking."""

    logger.info("A configurar o retriever...")
    k_for_reranker = config.get("retriever_k_for_reranker", 10)
    base_retriever = vector_store.as_retriever(search_kwargs={"k": k_for_reranker})
    
    logger.info(f"A carregar o modelo de re-ranking: {config['reranker_model_name']}")
    model = HuggingFaceCrossEncoder(model_name=config['reranker_model_name'])
    compressor = CrossEncoderReranker(model=model, top_n=3)
    
    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)
    logger.info("Retriever com re-ranker configurado com sucesso.")
    return compression_retriever

def create_rag_chain(llm, retriever, prompt):
    """Cria e retorna a cadeia RAG completa."""
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    return (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

@cl.set_starters
async def set_starters():
    """
    Define os starters (mensagens sugeridas) que aparecem no in√≠cio da conversa.
    Esta fun√ß√£o √© chamada automaticamente pelo Chainlit quando necess√°rio.
    """
    return [
        cl.Starter(
            label="üç§ Camar√µes grelhados",
            message="Preciso de uma sugest√£o de vinho branco com acidez e sem muito corpo para acompanhar uns camar√µes grelhados, pincelados com molho de manteiga, whisky e alho, um Alvarinho ou um Sauvignon Blanc ou um Verdelho ou um Riesling.",
        ),
        cl.Starter(
            label="üêü Ensopado de safio",
            message="Preciso de uma sugest√£o de vinho branco para acompanhar um ensopado de safio, um Verdelho ou um Arinto ou um Alvarinho, com pequeno est√°gio em barrica.",
        ),
        cl.Starter(
            label="üêñ Carne de porco com am√™ijoas",
            message="Um vinho tinto de lote sem madeira ou com est√°gio pequeno, n√£o muito alco√≥lico, um tinto mais jovem com castas t√≠picas do Alentejo, mas pode ser de qualquer outra regi√£o desde que tenha esta vinifica√ß√£o, para acompanhar uma carne de porco com am√™ijoas.",
        ),
        cl.Starter(
            label="ü¶ë Caldeirada de lulas",
            message="Um vinho branco com Alvarinho ou Loureiro ou Arinto para acompanhar caldeirada de lulas.",
        ),
        #cl.Starter(
        #    label="Ensopado de borredo",
        #    message="Um vinho tinto com Touriga Nacional para acompanhar um ensopado de borrego.",
        #),
        cl.Starter(
            label="üêü Raia de alhada",
            message="Um branco de Negra Mole ou um vinho branco com Verdelho ou com Arinto ou com Alvarinho para acompanhar raia de alhada.",
        ),
        cl.Starter(
            label="üêÑ Vitela estufada",
            message="Um vinho tinto com Syrah ou com Tannat ou com Malbec para acompanhar vitela estufada.",
        ),
        cl.Starter(
            label="ü¶Ü Am√™ijoas √° Bulh√£o Pato",
            message="Um vinho branco com Sauvignon Blanc ou Alvarinho ou Riesling para acompanhar am√™ijoas √° Bulh√£o Pato.",
        ),
        cl.Starter(
            label="ü•ö Ervilhas com ovos escalfados",
            message="Um vinho branco com Alvarinho ou com Crato Branco ou com Arinto ou com Ant√£o Vaz para acompanhar ervilhas com ovos escalfados.",
        ),
        cl.Starter(
            label="üêü Joaquinzinhos fritos",
            message="Um vinho branco Arinto ou Loureiro ou com Alvarinho ou com Crato Branco ou como Malvasia Fina para acompanhar uns joaquinzinhos fritos.",
        ),
        cl.Starter(
            label="üêö Feijoada de Buzinas",
            message="Um vinho tinto com Negra Mole para acompanhar uma feijoada com buzinas.",
        ),

    ]


# --- Chainlit ---

@cl.on_chat_start
async def on_chat_start():
    """
    Fun√ß√£o que √© executada quando um novo chat √© iniciado.
    Configura o pipeline RAG e armazena-o na sess√£o do utilizador.
    """

    logger = setup_logging()
    find_and_load_dotenv(logger)
    config = {**get_db_config(), **get_app_settings()}
    
    settings = await cl.ChatSettings(
        [
            Select(
                id="Model",
                label="Groq - Modelo",
                values=GROQ_MODELS,
                initial_index=0,
            )
        ]
    ).send()

    try:

        embedding_model = get_embedding_model(config, logger)
        vector_store = await get_async_vector_store(embedding_model, config, logger)

        if not vector_store:
            await cl.Message(content="Erro: N√£o foi poss√≠vel ligar √† base de dados de vinhos.").send()
            return
            
        retriever = get_retriever(vector_store, config, logger)
        prompt = get_rag_prompt()

        cl.user_session.set("retriever", retriever)
        cl.user_session.set("prompt", prompt)
        
        # Cria a cadeia inicial com o modelo default
        selected_model = settings["Model"]
        llm = get_groq_llm(selected_model, logger)
        rag_chain = create_rag_chain(llm, retriever, prompt)

        # Armazena a cadeia RAG na sess√£o do utilizador para ser usada em @cl.on_message
        cl.user_session.set("rag_chain", rag_chain)

    except Exception as e:
        logger.error(f"Ocorreu um erro durante a inicializa√ß√£o: {e}")
        await cl.Message(content=f"Ocorreu um erro cr√≠tico ao iniciar o chatbot: {e}").send()

@cl.on_settings_update
async def on_settings_update(settings):
    """
    Fun√ß√£o que √© executada sempre que o utilizador altera uma configura√ß√£o.
    """
    
    logger = setup_logging()
    selected_model = settings["Model"]
    
    logger.info(f"A atualizar para o modelo: {selected_model}")
    
    # Recupera os componentes da sess√£o
    retriever = cl.user_session.get("retriever")
    prompt = cl.user_session.get("prompt")
    
    # Cria um novo LLM e uma nova cadeia RAG com o modelo selecionado
    llm = get_groq_llm(selected_model, logger)
    rag_chain = create_rag_chain(llm, retriever, prompt)
    
    # Atualiza a cadeia na sess√£o do utilizador
    cl.user_session.set("rag_chain", rag_chain)
    
    await cl.Message(content=f"Modelo atualizado para `{selected_model}`.").send()


@cl.on_message
async def on_message(message: cl.Message):
    """
    Fun√ß√£o que √© executada sempre que o utilizador envia uma mensagem.
    """

    # Obt√©m a cadeia RAG que foi criada em on_chat_start
    rag_chain = cl.user_session.get("rag_chain")
    if not rag_chain:
        await cl.Message(content="O chatbot n√£o foi inicializado corretamente.").send()
        return

    # Cria uma mensagem vazia que ser√° preenchida com a resposta em streaming
    msg = cl.Message(content="")
    await msg.send()

    # Executa a cadeia RAG de forma ass√≠ncrona e faz stream da resposta
    response_stream = ""
    async for chunk in rag_chain.astream(message.content):
        response_stream += chunk
        await msg.stream_token(chunk)
    
    await msg.update()
